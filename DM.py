# -*- coding: utf-8 -*-
"""chayma1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MgPTg3c5vFz5_H3iQccsm5F-ZmFKr_ab
"""

#import
import pandas as pd
from sklearn.impute import SimpleImputer 
from sklearn.experimental import enable_iterative_imputer
from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn.metrics import  precision_score, recall_score, f1_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC 
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import KFold
from warnings import simplefilter
from sklearn.metrics import f1_score 
from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, VotingClassifier
from sklearn.model_selection import GridSearchCV

#test bech ntestiw bih train
test = pd.read_csv('Test.csv')
train = pd.read_csv('Train.csv')

#get dummies heya fn nbedel beha les chaine l a3ded
def get_dummies(df,test = False):
    df.Month = df['Month'].map({'Feb' : 2, 'Mar' : 3, 'May' : 5, 'Oct': 10, 'June' : 6, 'Jul' : 7, 'Aug' : 8, 'Nov' : 11, 'Sep' : 9,'Dec' : 12}).astype(int)
    df.VisitorType = df['VisitorType'].map({'Returning_Visitor' : 2, 'New_Visitor' : 1, 'Other' : 3}).astype(int)
    df.Weekend = df['Weekend'].map( {True: 1, False: 0} ).astype(int)
    if test == False:
        df.Revenue = df['Revenue'].map( {True: 1, False: 0} ).astype(int)

get_dummies(train)

#kif get dummies
def ReplacingTest(df,test = False):
    df.Month = df['Month'].map({
                                'Nov' : 11,
                                'Dec' : 12}).astype(int)
    df.VisitorType = df['VisitorType'].map({'Returning_Visitor' : 2, 'New_Visitor' : 1, 'Other' : 3}).astype(int)
    df.Weekend = df['Weekend'].map( {True: 1, False: 0} ).astype(int)

ReplacingTest(test)

#drop lel colonne tee l id fel train
train = train.drop(['id'], axis=1)

#dropna tnahi el nan elli houma des valeurs nulles 
test = test.dropna()
train = train.dropna()

#affiche 1 ou 0
train['Administrative_Duration'] = train['Administrative_Duration']/27.0 
train['PageValues'] = train ['PageValues']/360.953384
train['ProductRelated'] = train ['ProductRelated']/705.0
train['Informational_Duration'] = train ['Informational_Duration']/2549.375000
train['ProductRelated_Duration'] = train ['ProductRelated_Duration']/63973.522230

#affiche 1 ou 0
test['Administrative_Duration'] = test['Administrative_Duration']/27.0
test ['PageValues'] = test ['PageValues']/360.953384
test['ProductRelated'] = test ['ProductRelated']/705.0
test['Informational_Duration'] = test ['Informational_Duration']/2549.375000
test['ProductRelated_Duration'] = test ['ProductRelated_Duration']/63973.522230

#bech nakhtarou el X wel y mte3na bech naamlou behom prediction baad
X = train.drop(['Revenue'], axis=1) #x khdhet les colonnes l kol sauf Revenue
y = train[['Revenue']] #y khdhet l revenue
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.20 , random_state=0) #bech n9asemhom

#algorithmes de regression
knn = KNeighborsClassifier()
tree = DecisionTreeClassifier()
svm = SVC()
naive = GaussianNB()
rfc = RandomForestClassifier()

#estimation bel fit
knn.fit(X_train, y_train)
tree.fit(X_train, y_train)
naive.fit(X_train, y_train)
svm.fit(X_train, y_train)
rfc.fit(X_train, y_train)

#estimation de fiabilité d’un modèle
k_fold = KFold(n_splits= 10, shuffle = True, random_state=0)

#lahne khtarna l knn aala khater jena ahsen score
for k in range(3,15):
  knn = KNeighborsClassifier(n_neighbors=k)
  knn.fit(X_train, y_train)
  y_preds = knn.predict(X_test)

#ignorer les msg d erreur
simplefilter(action='ignore', category=FutureWarning)

gbc = GradientBoostingClassifier()
rfc = RandomForestClassifier()

gbc.fit(X_train, y_train)
rfc.fit(X_train, y_train)

#prediction aala chaque modele 
for model, name in [ [knn,'knn'], [tree,'tree'], [naive,'naive'], [svm,'svm'], [gbc, 'gbc'], [rfc,'rfc'] ]:
  y_preds = model.predict(X_test)

#khtarna gbc khater akther score
gbc = GradientBoostingClassifier()

param_grid = {
    "n_estimators": range(100,1000,200), 
    'criterion': ['gini', 'entropy']
}

grid = GridSearchCV(rfc, param_grid, scoring="accuracy", cv=2, n_jobs=-1)

param_grid = {"max_depth": [3, None],"max_features": [1, 3, 10],"min_samples_split": [2, 3, 10],"bootstrap": [True, False],"criterion": ["gini", "entropy"]}
rforest = RandomForestClassifier()
forest_cv = GridSearchCV(rforest,param_grid,cv=10 , n_jobs=-1) 
forest_cv = forest_cv.fit(X,y) 
forest_cv.best_params_

#drop lel colonne tee l id fel test
test = test.drop(['id'], axis=1)
#staamelna l get dummies aala test
get_dummies(test,test= True)

#hedha l fichier elli bech nsubmitiwah fel lekher
test2 = pd.read_csv('Test.csv')

#fichier lekher bech nhot fih l id wel prediction mteei khw
gbc.fit(X,y)
gbc.predict(test)

test2 =pd.DataFrame(test2 ['id'])

preds = gbc.predict(test)
#submission
submission = test2[['id']]
submission['Revenue'] = preds
submission.to_csv("submission_forest3.csv", index = False)